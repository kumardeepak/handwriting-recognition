{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import uuid\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have \"data\" directory in parallel to \"notebooks\". Create \"input\" directory under\n",
    "# \"data\" directory and copy sample image file.\n",
    "# application creates \"output\" programmatically.\n",
    "# ----+/notebooks\n",
    "# ----+/data\n",
    "# ----------+/input\n",
    "\n",
    "base_dir                   = '/Users/kd/Workspace/python/github/handwriting-recognition'\n",
    "data_dir                   = 'data'\n",
    "input_data_dir             = 'input'\n",
    "output_data_dir            = 'output'\n",
    "\n",
    "output_cropped_data_dir    = 'cropped'\n",
    "output_extracted_data_dir  = 'extracted'\n",
    "input_filename             = 'sample_input_01.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def create_directory(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "        return True\n",
    "    except FileExistsError as fe_error:\n",
    "        return True\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    return False\n",
    "\n",
    "# read file directory\n",
    "def read_directory(path, pattern='*'):\n",
    "    files = [f for f in glob.glob(os.path.join(path, pattern))]\n",
    "    return files\n",
    "\n",
    "def show_img(img):\n",
    "    plt.axis('off')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify number of tables present on the given image file\n",
    "def get_table_bounding_boxes(threshold, image):\n",
    "    kernel          = np.ones((3,3), 'uint8')\n",
    "    par_img         = cv2.dilate(threshold, kernel, iterations=2)\n",
    "    _, contours, _  = cv2.findContours(par_img.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "# save table images separately\n",
    "def get_relevant_images(img, contours, allowed_area=50000):\n",
    "    images = []\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        if (w*h) > allowed_area:\n",
    "            crop_img = img[y:y+h, x:x+w]\n",
    "            images.append(crop_img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def extract_save_tables(filepath, extraction_path):\n",
    "    image           = cv2.imread(filepath)\n",
    "    gray            = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, threshold  = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours        = get_table_bounding_boxes(threshold, image)\n",
    "    images          = get_relevant_images(image, contours)\n",
    "    print(\"found (%d) image of interest in the given filepath (%s)\"%(len(images), filepath))\n",
    "    for index, image in enumerate(images):\n",
    "        temp_filepath = os.path.join(extraction_path, \"extract_\" + str(index) + \".jpg\")\n",
    "        print(\"saving table at (%s)\"%(temp_filepath))\n",
    "        cv2.imwrite(temp_filepath, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect horizontal and vertical lines in the cropped images and extract boxes\n",
    "def smoothen_out_image(image):\n",
    "    edges  = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, -2)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    edges  = cv2.dilate(edges, kernel)\n",
    "    smooth = np.copy(image)\n",
    "    smooth = cv2.blur(smooth, (2, 2))\n",
    "    (rows, cols) = np.where(edges != 0)\n",
    "    image[rows, cols] = smooth[rows, cols]\n",
    "    return image\n",
    "\n",
    "def combine_image(vertical_img, horizontal_img):\n",
    "    alpha  = 0.5\n",
    "    beta   = 1.0 - alpha\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    # This function helps to add two image with specific weight parameter to get a third image as summation of two image.\n",
    "    img_final = cv2.addWeighted(vertical_img, alpha, horizontal_img, beta, 0.0)\n",
    "    img_final = cv2.erode(~img_final, kernel, iterations=2)\n",
    "    (thresh, img_final) = cv2.threshold(img_final, 128,255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return img_final\n",
    "\n",
    "def extract_boxes_from_image(filepath):\n",
    "    src_img      = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
    "    print(\"image channels (%d), converting to 2 channels\"%(len(src_img.shape)))\n",
    "    gray_img     = src_img\n",
    "    if len(src_img.shape) == 3:\n",
    "        gray_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray_img = cv2.bitwise_not(gray_img)\n",
    "    bw_img   = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, -2)\n",
    "    \n",
    "    horizontal_img = np.copy(bw_img)\n",
    "    vertical_img   = np.copy(bw_img)\n",
    "    \n",
    "    cols = horizontal_img.shape[1]\n",
    "    horizontal_size = cols // 30\n",
    "    horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n",
    "    horizontal_img = cv2.erode(horizontal_img, horizontalStructure)\n",
    "    horizontal_img = cv2.dilate(horizontal_img, horizontalStructure)\n",
    "    \n",
    "    rows = vertical_img.shape[0]\n",
    "    vertical_size = rows // 30\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n",
    "    vertical_img = cv2.erode(vertical_img, verticalStructure)\n",
    "    vertical_img = cv2.dilate(vertical_img, verticalStructure)\n",
    "    \n",
    "    horizontal_img = smoothen_out_image(horizontal_img)\n",
    "    vertical_img   = smoothen_out_image(vertical_img)\n",
    "    \n",
    "    img_final      = combine_image(vertical_img, horizontal_img)\n",
    "    return src_img, img_final\n",
    "\n",
    "def sort_contours(cnts, method=\"left-to-right\"):\n",
    "    # initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i = 0\n",
    "\n",
    "    # handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "\n",
    "    # handle if we are sorting against the y-coordinate rather than\n",
    "    # the x-coordinate of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "\n",
    "    # construct the list of bounding boxes and sort them from top to\n",
    "    # bottom\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "        key=lambda b:b[1][i], reverse=reverse))\n",
    "\n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (cnts, boundingBoxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input filename : [/Users/kd/Workspace/python/github/handwriting-recognition/data/input/sample_input_01.jpg]\n",
      "output working directory: [/Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01]\n",
      "output cropped directory: [/Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/cropped]\n",
      "output extracted directory: [/Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/extracted]\n"
     ]
    }
   ],
   "source": [
    "# program initialization \n",
    "img_filename    = os.path.join(base_dir, data_dir, input_data_dir, input_filename)\n",
    "print(\"input filename : [%s]\"%(img_filename))\n",
    "\n",
    "output_basedir  = os.path.join(base_dir, data_dir, output_data_dir, os.path.splitext(input_filename)[0])\n",
    "print(\"output working directory: [%s]\"%(output_basedir))\n",
    "ret = create_directory(os.path.join(base_dir, data_dir, output_data_dir))\n",
    "ret = create_directory(output_basedir)\n",
    "\n",
    "output_cropped_dir = os.path.join(output_basedir, output_cropped_data_dir)\n",
    "print(\"output cropped directory: [%s]\"%(output_cropped_dir))\n",
    "ret = create_directory(output_cropped_dir)\n",
    "\n",
    "output_extracted_dir = os.path.join(output_basedir, output_extracted_data_dir)\n",
    "print(\"output extracted directory: [%s]\"%(output_extracted_dir))\n",
    "ret = create_directory(output_extracted_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found (2) image of interest in the given filepath (/Users/kd/Workspace/python/github/handwriting-recognition/data/input/sample_input_01.jpg)\n",
      "saving table at (/Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/extracted/extract_0.jpg)\n",
      "saving table at (/Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/extracted/extract_1.jpg)\n"
     ]
    }
   ],
   "source": [
    "# execution\n",
    "extract_save_tables(img_filename, output_extracted_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image channels (3), converting to 2 channels\n",
      "processing contours for filename: /Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/extracted/extract_1.jpg\n",
      "image channels (3), converting to 2 channels\n",
      "processing contours for filename: /Users/kd/Workspace/python/github/handwriting-recognition/data/output/sample_input_01/extracted/extract_0.jpg\n"
     ]
    }
   ],
   "source": [
    "extracted_files = read_directory(output_extracted_dir)\n",
    "\n",
    "for filepath in extracted_files:\n",
    "    image, image_processed = extract_boxes_from_image(filepath)\n",
    "    # Find contours for image, which will detect all the boxes\n",
    "    _, contours, hierarchy = cv2.findContours(image_processed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Sort all the contours by top to bottom.\n",
    "    (contours, boundingBoxes) = sort_contours(contours, method='top-to-bottom')\n",
    "    print(\"processing contours for filename: %s\"%(filepath))\n",
    "    cont_ind = 0\n",
    "    for index, contour in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if (w > 80 and h > 20) and (w*h < 100000):\n",
    "            filename = os.path.join(output_cropped_dir, str(index)+\"_\"+str(int(cont_ind/5))+\"_\"+str(int(cont_ind%5))+\"_\"+os.path.basename(filepath))\n",
    "            crop_img = image[y:y+h, x:x+w]\n",
    "            cv2.imwrite(filename, crop_img)\n",
    "            cont_ind = cont_ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv3]",
   "language": "python",
   "name": "conda-env-cv3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
